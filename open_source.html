<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>WAVLab | Open-source</title>
<meta name="description" content="Webpage of Watanabe's Audio and Voice (WAV) Lab
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="/assets/img/favicon.png">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/open_source">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       WAVLab
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              Activities
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/members/">
                Members
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/open_source">
                Open-source
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sponsors/">
                Sponsors
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/info/">
                Info
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/speech_lunch">
                Speech Lunch
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Open-source</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <p>Our lab has led and participated in the development of several open-source toolkits, projects, and datasets. Some selected ones are listed below.</p>

<h3 id="softwares">Softwares</h3>

<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://github.com/espnet/espnet">
        <img src="/assets/img/espnet_logo1.png" width="150" />
      </a>
</td>
<td>
  <strong>ESPnet</strong> is an end-to-end speech processing toolkit, with a broad coverage of speech recognition, text-to-speech, speech enhancement/separation, and speech translation. ESPnet uses pytorch as a main deep learning engine, and also follows Kaldi style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25" style="display:table-cell; vertical-align:middle; text-align:center">
      <a href="https://github.com/kaldi-asr/kaldi">
        <img src="/assets/img/kaldi-logo.png" width="100" />
      </a>
</td>
<td>
  <strong>Kaldi</strong> is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://github.com/s3prl/s3prl">
        <img src="/assets/img/S3PRL-logo.png" width="150" />
      </a>
</td>
<td>
  This is an open source toolkit called <strong>s3prl</strong>, which stands for <strong>S</strong>elf-<strong>S</strong>upervised <strong>S</strong>peech <strong>P</strong>re-training and <strong>R</strong>epresentation <strong>L</strong>earning. Self-supervised speech pre-trained models are called <strong>upstream</strong> in this toolkit, and are utilized in various <strong>downstream</strong> tasks.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25" style="display:table-cell; vertical-align:middle; text-align:center">
      <a href="https://github.com/freewym/espresso">
        <img src="/assets/img/espresso_logo.png" width="100" />
      </a>
</td>
<td>
  <strong>Espresso</strong> is an open-source, modular, extensible end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. Espresso supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25" style="display:table-cell; vertical-align:middle; text-align:center">
      <a href="https://github.com/SJTMusicTeam/Muskits">
        <img src="/assets/img/muskit_logo.png" width="200" />
      </a>
</td>
<td>
  <strong>Muskits</strong>  is an open-source music processing toolkit, currently focus on benchmarking the end-to-end singing voice synthesis and expect to extend more tasks in the future. Muskit employs pytorch as a deep learning engine and also follows ESPnet and Kaldi style data processing, and recipes to provide a complete setup for various music processing experiments.
</td></tr>
</table>
<hr />

<h3 id="projects">Projects</h3>

<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="/activities/2024/owsm/">
        OWSM
      </a>
</td>
<td>
  <strong>Open Whisper-style Speech Models</strong> (<strong>OWSM</strong>, pronounced as "awesome") are a series of speech foundation models developed by WAVLab at Carnegie Mellon University. We reproduce Whisper-style training using publicly available data and our open-source toolkit ESPnet. By publicly releasing data preparation scripts, training and inference code, pre-trained model weights and training logs, we aim to promote transparency and open science in large-scale speech pre-training.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="/activities/2024/xeus/">
        XEUS
      </a>
</td>
<td>
  <strong>XEUS</strong> (pronounced “Zeus”) is an open-source speech foundation model trained on nearly 1.1 million hours of unlabeled speech data across 4,057 languages. XEUS sets the new state-of-the-art on the ML-SUPERB multilingual speech recognition benchmark, while also achieving strong results on different tasks on the English-only SUPERB evaluations. We open-source XEUS’ checkpoints, along with our training code and 4,000+ language speech data in this project page.
</td></tr>
</table>
<hr />

<h3 id="datasets">Datasets</h3>

<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://chimechallenge.github.io/chime6/">
        <img src="/assets/img/chime-logo.png" width="150" />
      </a>
</td>
<td>
  Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges, we are pleased to announce the 6th <strong>CHiME</strong> Speech Separation and Recognition Challenge (CHiME-6). The new challenge will consider distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://github.com/mispchallenge/MISP2021-AVSR">
        AVSR (MIPS2021)
      </a>
</td>
<td>
  <strong>Audio-Visual Speech Recognition (AVSR)</strong> corpus of MISP2021 challenge, a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. To our best knowledge, our corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous Chinese lip-reading dataset in the adverse home-tv scenario.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://github.com/mispchallenge/MISP2021-AVWWS">
        AVWWS (MIPS2021)
      </a>
</td>
<td>
  <strong>Audio-Visual Wake Word Spotting (AVWWS)</strong> concerns the identification of predefined wake word(s) in utterances. ‘1’ indicates that the sample contains wake word, and ‘0’ indicates the opposite. For more information, please refer to the MISP Challenge task 1 description.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://datasets.kensho.com/datasets/spgispeech">
        SPGISpeech
      </a>
</td>
<td>
  <strong>SPGISpeech</strong> is a corpus of 5,000 hours of professionally-transcribed financial audio. In contrast to previous transcription datasets, SPGISpeech contains a broad cross-section of L1 and L2 English accents, strongly varying audio quality, and both spontaneous and narrated speech. The transcripts have each been cross-checked by multiple professional editors for high accuracy and are fully formatted, including capitalization, punctuation, and denormalization of non-standard words.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="https://github.com/SpeechColab/GigaSpeech">
        GigaSpeech
      </a>
</td>
<td>
  <strong>GigaSpeech</strong> is an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="http://www.openslr.org/89/">
        ASR corpus for endangered language documentation (Yoloxóchitl Mixtec)
      </a>
</td>
<td>
  Substantive material of <strong>Yoloxóchitl Mixtec</strong> speech corpus (Glottocode: yolo1241 | ISO 639-3 = xty) presented here was brought together over ten years by Jonathan D. Amith (PI) and Rey Castillo García, a native speaker linguist from the community of Yoloxóchitl. The corpus is designed for ASR research in endangered language documentation.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="http://www.openslr.org/89/">
        ASR and ST corpus for endangered language documentation (Puebla Nahuatl)
      </a>
</td>
<td>
  The substantive material of <strong>Puebla Nahuatl</strong> speech corpus was gathered over ten years by Jonathan D. Amith (PI) and a team of native-speaker colleagues who have participated in the project for many years, one from its inception in 2009. The corpus is designed for ASR &amp; MT research in endangered language documentation.
</td></tr>
</table>
<hr />

<table cellspacing="0" cellpadding="0">
<tr>
<td class="col-sm w-25">
      <a href="http://www.openslr.org/107/">
        ASR corpus for endangered language documentation (Totonac)
      </a>
</td>
<td>
  The substantive material of <strong>Totonac</strong> from the northern sierras of Puebla and adjacent areas of Veracruz were compiled starting in 2016 by Jonathan D. Amith and continue to the present as part of a joint effort by Amith and Osbel López Francisco, a native speaker biologist from Zongozotla.
</td></tr>
</table>
<hr />


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025   WAV Lab.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
