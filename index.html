<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>WAVLab</title>
<meta name="description" content="Webpage of Watanabe's Audio and Voice (WAV) Lab
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="/assets/img/favicon.png">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       WAVLab
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              Activities
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/members/">
                Members
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/open_source">
                Open-source
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sponsors/">
                Sponsors
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/info/">
                Info
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/speech_lunch">
                Speech Lunch
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     WAVLab
    </h1>
     <p class="desc">affiliated with @ <a href="https://www.lti.cs.cmu.edu/">LTI/CMU</a>.</p>
  </header>

  <article>
    

    <div class="clearfix">
      <p>This is Watanabe’s Audio and Voice (WAV) Lab at the Language Technologies Institute of Carnegie Mellon University. Our research interests include automatic speech recognition, speech enhancement, spoken language understanding, and machine learning for speech and language processing.</p>

<div class="col-sm mt-3 mt-md-0" style="display:table-cell; vertical-align:middle; text-align:center">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/PXL_20240506_150609437.MP.jpg" />
    <div class="caption">
        The end-of-semester presentation, 05.06.2024
    </div>
</div>

<!-- <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-tweet-limit="3" href="https://twitter.com/WavLab?ref_src=twsrc%5Etfw">Tweets by WavLab</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div> -->

<!-- # Tweet
An example of displaying a tweet:
<div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div> -->

    </div>

<!--     
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Jun 15, 2022</th>
          <td>
            
              Our lab has 23 paper accepted in the Interspeech2022. Detailed list will be available in <a href="/publications">publication page</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 1, 2022</th>
          <td>
            
              Our lab has 18 ICASSP paper accepted in the ICASSP2022. Detailed list is already available in <a href="/publications">publication page</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 13, 2021</th>
          <td>
            
              Our lab has 9 ASRU paper accepted in the ASRU2021. Detailed list is already available in <a href="/publications">publication page</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 7, 2021</th>
          <td>
            
              Shinji, with Keisuke, Yusuke, and Naoyuki, delivered a tutorial on “Distant Conversational Speech Recognition And Analysis: Recent Advances, And Trends Towards End-To-End Optimization” in ICASSP 2021. Detailed slides can be found <a href="https://github.com/ICASSP2021-tutorial9/Distant_conversational_ASR_and_analysis">here</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 3, 2021</th>
          <td>
            
              Our lab has 20 Interspeech paper accepted in the Interspeech2021. Detailed list will be available soon in <a href="/publications">publication page</a>.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

     -->

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SD</abbr>
    
  
  
    
    <abbr class="badge badge-info">CSL</abbr>
    
  
  </div>

  <div id="park2022review" class="col-sm-8">
    
      <div class="title">A review of speaker diarization: Recent advances with deep learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Tae Jin Park,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Naoyuki Kanda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dimitrios Dimitriadis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kyu J Han,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shrikanth Narayanan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computer Speech & Language</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ASR&SD&SLU&ER</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="yang2021superb" class="col-sm-8">
    
      <div class="title">SUPERB: Speech processing Universal PERformance Benchmark</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Shu-wen Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Po-Han Chi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yung-Sung Chuang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cheng-I Lai,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kushal Lakhotia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yist Y.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Andy T.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://shijt.site" target="_blank">Jiatong Shi</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://simpleoier.github.io" target="_blank">Xuankai Chang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Guan-Ting Lin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tzu-Hsien Huang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wei-Cheng Tseng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ko-tik Lee,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Da-Rong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zili Huang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shuyan Dong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shang-Wen Li,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Abdelrahman Mohamed,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hung-yi Lee
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of Interspeech</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
      <a href="http://arxiv.org/abs/2105.01051" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/2105.01051.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TTS</abbr>
    
  
  
    
    <abbr class="badge badge-info">ICASSP</abbr>
    
  
  </div>

  <div id="hayashi2020espnet" class="col-sm-8">
    
      <div class="title">Espnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Tomoki Hayashi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ryuichi Yamamoto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Katsuki Inoue,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Takenori Yoshimura,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tomoki Toda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kazuya Takeda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yu Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Xu Tan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
      <a href="http://arxiv.org/abs/1910.10909" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/9053512/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/1910.10909.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ST</abbr>
    
  
  
    
    <abbr class="badge badge-info">ACL</abbr>
    
  
  </div>

  <div id="inaguma-etal-2020-espnet" class="col-sm-8">
    
      <div class="title">ESPnet-ST: All-in-One Speech Translation Toolkit</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hirofumi Inaguma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shun Kiyono,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kevin Duh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shigeki Karita,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nelson Yalta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tomoki Hayashi,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
      <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SD</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="horiguchi2020end" class="col-sm-8">
    
      <div class="title">End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Shota Horiguchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yusuke Fujita,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yawen Xue,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kenji Nagamatsu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
      <a href="http://arxiv.org/abs/2005.09921" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/hitachi-speech/EEND" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SD</abbr>
    
  
  
    
    <abbr class="badge badge-info">ASRU</abbr>
    
  
  </div>

  <div id="fujita2019end" class="col-sm-8">
    
      <div class="title">End-to-end neural speaker diarization with self-attention</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yusuke Fujita,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Naoyuki Kanda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shota Horiguchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yawen Xue,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kenji Nagamatsu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE Automatic Speech Recogiton and Understanding Workshop (ASRU)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
      <a href="http://arxiv.org/abs/1909.06247" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/9003959" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/1909.06247.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SD</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="fujita19inter" class="col-sm-8">
    
      <div class="title">End-to-End Neural Speaker Diarization with Permutation-Free Objectives</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yusuke Fujita,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Naoyuki Kanda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shota Horiguchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kenji Nagamatsu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of Interspeech</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/2899.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2899.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ASR</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="karita19inter" class="col-sm-8">
    
      <div class="title">Improving Transformer Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Shigeki Karita,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nelson Yalta,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Marc Delcroix,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Atsunori Ogawa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Tomohiro Nakatani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of Interspeech</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1938.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/1938.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ASR</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="watanabe2018espnet" class="col-sm-8">
    
      <div class="title">ESPnet: End-to-End Speech Processing Toolkit</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Takaaki Hori,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shigeki Karita,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tomoki Hayashi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiro Nishitoba,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuya Unno,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nelson Enrique Yalta Soplin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jahn Heymann,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Matthew Wiesner,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nanxin Chen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Adithya Renduchintala,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Tsubasa Ochiai
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of Interspeech</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1804.00015" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://www.isca-speech.org/archive/interspeech_2018/watanabe18_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/watanabe18_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SE&ASR</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="barker2018fifth" class="col-sm-8">
    
      <div class="title">The Fifth ’CHiME’ Speech Separation and Recognition Challenge: Dataset, Task and Baselines</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jon Barker,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Emmanuel Vincent,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jan Trmal
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of Interspeech</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
    
      <a href="http://arxiv.org/abs/1803.10609" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
      <a href="https://www.isca-speech.org/archive/interspeech_2018/barker18_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/barker18_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SD</abbr>
    
  
  
    
    <abbr class="badge badge-info">Interspeech</abbr>
    
  
  </div>

  <div id="sell2018diarization" class="col-sm-8">
    
      <div class="title">Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge.</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Gregory Sell,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  David Snyder,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Alan McCree,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daniel Garcia-Romero,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jesús Villalba,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Matthew Maciejewski,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Vimal Manohar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Najim Dehak,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daniel Povey,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and  others
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Interspeech</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.isca-speech.org/archive/interspeech_2018/sell18_interspeech.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/sell18_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe in this paper the experiences of the Johns Hopkins University team during the inaugural DIHARD diarization evaluation. This new task provided microphone recordings in a variety of difficult conditions and challenged researchers to fully consider all speaker activity, without the currently typical practices of unscored collars or ignored overlapping speaker segments. This paper explores several key aspects of currently state-of-the-art diarization methods, such as training data selection, signal bandwidth for feature extraction, representations of speech segments (i-vector versus x-vector) and domain-adaptive processing. In the end, our best system clustered x-vector embeddings trained on wideband microphone data followed by Variational-Bayesian refinement and a speech activity detector specifically trained for this task with in-domain data was found to be the best performing. After presenting these decisions and their final result, we discuss lessons learned and remaining challenges within the lens of this new approach to diarization performance measurement.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        




<a href="https://github.com/shinjiwlab" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/company/wavlab-cmu" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/WavLab" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>









      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=s2kcUarMcOgurEYClUOjmjuZwmzMNsQOUdYJfD1PwGY&cl=ffffff&w=a"></script>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025   WAV Lab.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
