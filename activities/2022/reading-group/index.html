<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>WAVLab | 2022 Reading Group</title>
<meta name="description" content="Webpage of Watanabe's Audio and Voice (WAV) Lab
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="/assets/img/favicon.png">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/activities/2022/reading-group/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       WAVLab
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              Activities
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/members/">
                Members
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/open_source">
                Open-source
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sponsors/">
                Sponsors
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/info/">
                Info
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/speech_lunch">
                Speech Lunch
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">2022 Reading Group</h1>
    <p class="post-meta">January 28, 2022</p>
  </header>

  <article class="post-content">
    <h3 id="20211221-asru-2021-paper-list">2021.12.21 ASRU 2021 Paper List</h3>
<ul>
  <li><a href="http://sap.ist.i.kyoto-u.ac.jp/EN/bib/intl/UEN-ASRU21.pdf">Data Augmentation for ASR Using TTS via A Discrete Representation</a></li>
  <li><a href="https://arxiv.org/pdf/2109.01163.pdf">Efficient Conformer: Progressive Downsampling and Grouped Attention for Automatic Speech Recognition</a></li>
</ul>

<h3 id="20220111-asru-2021-paper-list">2022.01.11 ASRU 2021 Paper List</h3>
<ul>
  <li>Improving HS-DACS Based Streaming Transformer ASR with Deep Reinforcement Learning</li>
  <li><a href="https://arxiv.org/pdf/2108.07789.pdf">Adapting GPT, GPT-2 and Bert Language Models for Speech Recognition</a></li>
  <li><a href="https://arxiv.org/pdf/2103.16804.pdf">TS-RIR: Translated Synthetic Room Impulse Responses for Speech Augmentation</a></li>
</ul>

<h3 id="20220119-asru-2021-paper-list">2022.01.19 ASRU 2021 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2109.05494.pdf">Unsupervised Domain Adaptation Schemes for Building ASR in Low-Resource Languages</a></li>
  <li><a href="https://arxiv.org/pdf/2107.01275.pdf">Relaxed Attention: A Simple Method to Boost Performance of End-To-End Automatic Speech Recognition</a></li>
</ul>

<h3 id="20220202-asru-2021-paper-list">2022.02.02 ASRU 2021 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2106.07759.pdf">Kaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition</a></li>
  <li><a href="https://arxiv.org/pdf/2104.05379.pdf">Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures</a></li>
  <li><a href="https://arxiv.org/pdf/2109.06103.pdf">Joint prediction of truecasing and punctuation for conversational speech in low-resource scenarios</a></li>
</ul>

<h3 id="20220216-survey-of-streaming-slu-presented-by-siddhant-arora">2022.02.16 Survey of Streaming SLU (presented by Siddhant Arora)</h3>
<ul>
  <li><a href="https://github.com/shinjiwlab/shinjiwlab.github.io/tree/source/assets/pdf/2022-02-16_Siddhant_Streaming_SLU.pdf">Streaming SLU and Wake-up Word Detection</a></li>
</ul>

<h3 id="20220223-neurips-2021-paper-list">2022.02.23 NeurIPS 2021 Paper List</h3>
<ul>
  <li><a href="https://proceedings.neurips.cc/paper/2021/hash/ea159dc9788ffac311592613b7f71fbb-Abstract.html">Unsupervised Speech Recognition</a></li>
  <li><a href="https://proceedings.neurips.cc/paper/2021/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html">A Universal Law of Robustness via Isoperimetry</a></li>
  <li><a href="https://proceedings.neurips.cc/paper/2021/hash/344ef5151be171062f42f03e69663ecf-Abstract.html">Speech-T: Transducer for Text to Speech and Beyond</a></li>
</ul>

<h3 id="20220302-neurips-2021-paper-list">2022.03.02 NeurIPS 2021 Paper List</h3>
<ul>
  <li><a href="https://openreview.net/forum?id=6fmgB38rLI1">Multimodal and Multilingual Embeddings for Large-Scale Speech Mining</a></li>
  <li><a href="https://openreview.net/forum?id=KBnXrODoBW">Pay Attention to MLPs</a></li>
  <li><a href="https://openreview.net/forum?id=SlxH2AbBBC2">Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network</a></li>
</ul>

<h3 id="20220330-neurips-2021-paper-list">2022.03.30 NeurIPS 2021 Paper List</h3>
<ul>
  <li><a href="https://openreview.net/pdf?id=N3oi7URBakV">FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition</a></li>
  <li><a href="https://nips.cc/virtual/2021/poster/37777">Neural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations</a></li>
  <li><a href="https://openreview.net/pdf?id=R6U4-Qkcg21">Unsupervised Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal Transport</a></li>
</ul>

<h3 id="20220406-adapters-in-speech-transformers-presented-by-karthik-ganesan">2022.04.06 Adapters in Speech Transformers (presented by Karthik Ganesan)</h3>
<ul>
  <li><a href="https://github.com/shinjiwlab/shinjiwlab.github.io/tree/source/assets/pdf/2022-04-06-Adapters.pdf">Adapters in Speech Transformers</a></li>
</ul>

<h3 id="20220413-neurips-2021-paper-list">2022.04.13 NeurIPS 2021 Paper List</h3>
<ul>
  <li><a href="https://nips.cc/virtual/2021/poster/28735">Understanding Adaptive, Multiscale Temporal Integration In Deep Speech Recognition Systems</a></li>
  <li><a href="https://nips.cc/virtual/2021/poster/26058">Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling</a></li>
  <li><a href="https://nips.cc/virtual/2021/workshop/21839#wse-detail-34284">Towards efficient end-to-end speech recognition with biologically-inspired neural networks</a></li>
</ul>

<h3 id="20220427-survey-of-semi-supervised-asr-presented-by-dan-berrebbi">2022.04.27 Survey of Semi-Supervised ASR (presented by Dan Berrebbi)</h3>
<ul>
  <li><a href="https://github.com/shinjiwlab/shinjiwlab.github.io/tree/source/assets/pdf/dan_reading_group_semi_supervised_asr.pdf">Semi-Supervised ASR</a></li>
</ul>

<h3 id="20220504-the-voicemos-challenge-2022-presented-by-wen-chin-huang-from-nagoya-university">2022.05.04 The VoiceMOS Challenge 2022 (presented by Wen-Chin Huang from Nagoya University)</h3>
<ul>
  <li><a href="https://github.com/shinjiwlab/shinjiwlab.github.io/tree/source/assets/pdf/2022-05-04-VoiceMOS-Challenge.pdf">The VoiceMOS Challenge 2022</a></li>
  <li><a href="https://codalab.lisn.upsaclay.fr/competitions/695">CodaLab Challenge Page</a></li>
  <li><a href="https://arxiv.org/abs/2203.11389">Paper</a></li>
  <li><a href="https://github.com/nii-yamagishilab/mos-finetune-ssl">Baseline System 1</a></li>
  <li><a href="https://github.com/dhimasryan/MOSA-Net-Cross-Domain">Baseline System 2</a></li>
  <li><a href="https://github.com/unilight/LDNet">Baseline System 3</a></li>
</ul>

<h3 id="20220601-icassp-2022-paper-list">2022.06.01 ICASSP 2022 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2110.05745">VarArray: Array-Geometry-Agnostic Continuous Speech Separation</a></li>
  <li><a href="https://ieeexplore.ieee.org/document/9747259">Self Supervised Representation Learning with Deep Clustering for Acoustic Unit Discovery from Raw Speech</a></li>
  <li><a href="https://arxiv.org/abs/2110.10757">TPARN: Triple-path Attentive Recurrent Network for Time-domain Multichannel Speech Enhancement</a></li>
</ul>

<h3 id="20220608-icassp-2022-paper-list">2022.06.08 ICASSP 2022 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2110.10330">One Model to Enhance Them All: Array Geometry Agnostic Multi-Channel Personalized Speech Enhancement</a></li>
  <li><a href="https://arxiv.org/abs/2202.03484">Self-supervised Speaker Recognition Training Using Human-Machine Dialogues</a></li>
  <li><a href="https://ieeexplore.ieee.org/document/9746798">Self-Supervised Learning Method Using Multiple Sampling Strategies for General-Purpose Audio Representation</a></li>
</ul>

<h3 id="20220615-icassp-2022-paper-list">2022.06.15 ICASSP 2022 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2110.15836">Combining Unsupervised and Text Augmented Semi-Supervised Learning for Low Resourced Autoregressive Speech Recognition</a></li>
  <li><a href="https://arxiv.org/abs/2110.01500">Factorized Neural Transducer for Efficient Language Model Adaptation</a></li>
  <li><a href="https://arxiv.org/abs/2203.16838">Neufa: Neural Network Based End-to-End Forced Alignment with Bidirectional Attention Mechanism</a></li>
  <li><a href="https://arxiv.org/abs/2203.00006">Towards Reducing the Need for Speech Training Data To Build Spoken Language Understanding Systems</a></li>
</ul>

<h3 id="20220727-icml-2022-paper-list">2022.07.27 ICML 2022 Paper List</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v162/zhang22i.html">Revisiting End-to-End Speech-to-Text Translation From Scratch</a></li>
  <li><a href="https://proceedings.mlr.press/v162/jia22b.html">Translatotron 2: High-quality direct speech-to-speech translation with voice preservation</a></li>
  <li><a href="https://proceedings.mlr.press/v162/huang22j.html">Efficient Representation Learning via Adaptive Context Pooling</a></li>
  <li><a href="https://proceedings.mlr.press/v162/wu22d.html">Characterizing and Overcoming the Greedy Nature of Learning in Multi-modal Deep Neural Networks</a></li>
  <li><a href="https://proceedings.mlr.press/v162/du22c.html">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</a></li>
</ul>

<h3 id="20220803-icml-2022-paper-list">2022.08.03 ICML 2022 Paper List</h3>
<ul>
  <li><a href="https://proceedings.mlr.press/v162/qian22b.html">ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers</a></li>
  <li><a href="https://proceedings.mlr.press/v162/bai22d.html">A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing</a></li>
  <li><a href="https://proceedings.mlr.press/v162/kim22d.html">Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance</a></li>
  <li><a href="https://proceedings.mlr.press/v162/zeng22a.html">Multi Resolution Analysis (MRA) for Approximate Self-Attention</a></li>
</ul>

<h3 id="20220810-icassp-2022-paper-list">2022.08.10 ICASSP 2022 Paper List</h3>
<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9746579">Consistent Training and Decoding for End-to-End Speech Recognition Using Lattice-Free MMI</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9746948">On Language Model Integration for RNN Transducer Based Speech Recognition</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/9746234">Tight Integration Of Neural- And Clustering-Based Diarization Through Deep Unfolding Of Infinite Gaussian Mixture Model</a></li>
</ul>

<h3 id="20220824-icassp-2022-paper-list">2022.08.24 ICASSP 2022 Paper List</h3>
<ul>
  <li><a href="https://ieeexplore.ieee.org/document/9746054">Spatial-Temporal Graph Convolution Network for Multichannel Speech Enhancement</a></li>
  <li><a href="https://ieeexplore.ieee.org/document/9746721">Exploring Machine Speech Chain For Domain Adaptation</a></li>
  <li><a href="https://arxiv.org/abs/2203.03582">Improving CTC-based speech recognition via knowledge transferring from pre-trained language models</a></li>
</ul>

<h3 id="2022113-interspeech-2022-paper-list">2022.11.3 INTERSPEECH 2022 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2110.03098">CTC Variations Through New WFST Topologies</a></li>
  <li><a href="https://arxiv.org/abs/2205.00693">Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding</a></li>
  <li><a href="https://arxiv.org/abs/2203.15796">Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition</a></li>
</ul>

<h3 id="20221110-interspeech-2022-paper-list">2022.11.10 INTERSPEECH 2022 Paper List</h3>
<ul>
  <li><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/sato22_interspeech.pdf">Text-Only Domain Adaptation Based on Intermediate CTC</a></li>
  <li><a href="https://arxiv.org/pdf/2206.12638.pdf">Distilling a Pretrained Language Model to a Multilingual ASR Model</a></li>
  <li><a href="https://arxiv.org/abs/2207.06867">Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models</a></li>
</ul>

<h3 id="2022121-interspeech-2022-paper-list">2022.12.1 INTERSPEECH 2022 Paper List</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2204.01893">Deliberation Model for On-Device Spoken Language Understanding</a></li>
  <li><a href="https://www.isca-speech.org/archive/interspeech_2022/kanda22b_interspeech.html">Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings</a></li>
  <li><a href="https://www.isca-speech.org/archive/interspeech_2022/bando22_interspeech.html">Weakly-Supervised Neural Full-Rank Spatial Covariance Analysis for a Front-End System of Distant Speech Recognition</a></li>
</ul>

<h3 id="2022128-neurips-2022-paper-list">2022.12.8 NeurIPS 2022 Paper List</h3>
<ul>
  <li><a href="https://openreview.net/forum?id=monPF76G5Uv">Global Normalization for Streaming Speech Recognition in a Modular Framework</a></li>
  <li><a href="https://openreview.net/forum?id=2EUJ4e6H4OX">Losses Can Be Blessings: Routing Self-Supervised Speech Representations Towards Efficient Multilingual and Multitask Speech Processing</a></li>
  <li><a href="https://openreview.net/forum?id=gE_vt-w4LhL">Squeezeformer: An Efficient Transformer for Automatic Speech Recognition</a></li>
</ul>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025   WAV Lab.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
