<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>WAVLab | Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge</title>
<meta name="description" content="Webpage of Watanabe's Audio and Voice (WAV) Lab
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="/assets/img/favicon.png">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/activities/2024/Interspeech2024-Discrete-Speech-Unit-Challenge/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       WAVLab
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              Activities
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/members/">
                Members
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/open_source">
                Open-source
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sponsors/">
                Sponsors
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/info/">
                Info
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/speech_lunch">
                Speech Lunch
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge</h1>
    <p class="post-meta">January 19, 2024</p>
  </header>

  <article class="post-content">
    <!---
### The Challenge Overview

Description here
--->

<h2 id="introduction">Introduction</h2>

<p>In conventional speech processing approaches, models typically take either raw waveforms or high-dimensional features derived from these waveforms as input. For instance, spectral speech features continue to be widely employed, while learning-based deep neural network features have gained prominence in recent years. A promising alternative arises in the form of discrete speech representation, where speech signals within a temporal window can be represented by a discrete token as shown in this <a href="https://arxiv.org/pdf/2309.15800.pdf">work</a>.</p>

<p>Three challenging tasks are proposed for using discrete speech representations.</p>
<ol>
  <li>Automatic speech recognition (ASR): We will evaluate the ASR performance of the proposed systems on the proposed data.</li>
  <li>Text-to-speech (TTS): We will evaluate the quality of the generated speech.</li>
  <li>Singing voice synthesis (SVS): We will evaluate the quality of the synthesized singing voice.</li>
</ol>

<p>Participation is open to all. Each team can participate in any task. This challenge has preliminarily been accepted as a special session for Interspeech 2024, and participants are strongly encouraged to submit papers to the special session. The focus of the special session is to promote the adoption of discrete speech representations and encourage novel insights.</p>

<!---
### Resources
--->

<h2 id="resources">Resources</h2>

<h3 id="baseline-systems--toolkits">Baseline systems &amp; toolkits</h3>
<ul>
  <li><a href="https://github.com/espnet/espnet/tree/master/egs2/interspeech2024_dsu_challenge/asr2">Automatic speech recognition (ASR)</a>
    <ul>
      <li>Results
        <ul>
          <li>WER is computed on English test sets (dev-clean / dev-other / test-clean / test-other)</li>
          <li>CER is computed on the multi-lingual test set (test_1h)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<table class="table">
  <thread>
    <tr>
      <th scope="col">Model</th>
      <th scope="col">dev-clean (LS)</th>
      <th scope="col">dev-other (LS)</th>
      <th scope="col">test-clean (LS)</th>
      <th scope="col">test-other (LS)</th>
      <th scope="col">test-1h (ML-SUPERB)</th>
    </tr>
  </thread>
  <tbody>
    <tr>
      <th scope="col">Wavlm-large-layer21</th>
      <th scope="col">4.5</th>
      <th scope="col">8.1</th>
      <th scope="col">4.4</th>
      <th scope="col">8.3</th>
      <th scope="col">72.6</th>
    </tr>
  </tbody>
</table>
<ul>
  <li><a href="https://github.com/espnet/espnet/tree/master/egs2/ljspeech/tts2">Text-to-speech (TTS)</a>
    <ul>
      <li>Results</li>
    </ul>
  </li>
</ul>
<table class="table">
  <thread>
    <tr>
      <th scope="col">Model</th>
      <th scope="col">MCD</th>
      <th scope="col">Log F0 RMSE</th>
      <th scope="col">WER</th>
      <th scope="col">UTMOS</th>
    </tr>
  </thread>
  <tbody>
    <tr>
      <th scope="col">HuBERT-base-layer6</th>
      <th scope="col">7.19</th>
      <th scope="col">0.26</th>
      <th scope="col">8.1</th>
      <th scope="col">3.73</th>
    </tr>
  </tbody>
</table>
<ul>
  <li><a href="https://github.com/A-Quarter-Mile/espnet/tree/tmp_muskit/egs2/opencpop/svs2">Singing voice synthesis (SVS)</a></li>
</ul>
<table class="table">
  <thread>
    <tr>
      <th scope="col">Model</th>
      <th scope="col">MCD</th>
      <th scope="col">Log F0 RMSE</th>
    </tr>
  </thread>
  <tbody>
    <tr>
      <th scope="col">WavLM-large-layer6</th>
      <th scope="col">8.47</th>
      <th scope="col">0.18</th>
    </tr>
  </tbody>
</table>
<ul>
  <li><a href="https://github.com/kan-bayashi/ParallelWaveGAN">Discrete vocoder training</a></li>
</ul>
<table class="table">
  <thread>
    <tr>
      <th scope="col">Model</th>
      <th scope="col">MCD</th>
      <th scope="col">Log F0 RMSE</th>
      <th scope="col">UTMOS</th>
    </tr>
  </thread>
  <tbody>
    <tr>
      <th scope="col">HuBERT-base-layer6</th>
      <th scope="col">7.19</th>
      <th scope="col">0.42</th>
      <th scope="col">2.27</th>
    </tr>
  </tbody>
</table>

<h3 id="track-specific-dataset">Track-specific dataset</h3>

<ul>
  <li>ASR: <a href="https://www.openslr.org/12">Librispeech</a> and <a href="https://drive.google.com/file/d/1zslKQwadZaYWXAmfBCvlos9BVQ9k6PHT/view?usp=sharing">ML-SUPERB</a></li>
  <li>TTS: <a href="https://keithito.com/LJ-Speech-Dataset/">LJSpeech</a> and <a href="https://speechbot.github.io/expresso/">Expresso</a></li>
  <li>SVS: <a href="https://wenet.org.cn/opencpop/">Opencpop</a></li>
</ul>

<h3 id="data-for-discrete-representation-learning-and-extraction">Data for discrete representation learning and extraction</h3>
<ul>
  <li>
    <p>General Policy: There are no restrictions on using datasets for learning and extracting discrete representations. This applies broadly to all datasets.</p>
  </li>
  <li>
    <p>Specific Restrictions for Supervision Data: The key restriction is on using test sets from certain datasets for supervision in specific tasks. Specifically:</p>
    <ul>
      <li>Automatic Speech Recognition (ASR): The test sets of the Librispeech and ML-SUPERB datasets cannot be used for learning the discrete representation. However, their training sets are permissible.</li>
      <li>Text-to-Speech (TTS): The test sets of the LJSpeech and Expresso datasets are off-limits for discrete representation learning, but their training sets can be used. For TTS training, phone alignment information for non-autoregressive training can be also used in the training phase.</li>
      <li>Singing Voice Synthesis (SVS): The test set of the Opencpop dataset is restricted for use in discrete representation learning, though the training set is allowed.</li>
    </ul>
  </li>
</ul>

<!-- ### Rules
* For each task, the training data must follow the baseline systems. However, there is no constraint on the data used in the foundation models.
* For submission, more details will be provided later for each task.
 -->

<h2 id="detailed-tracks-and-rules">Detailed tracks and rules</h2>

<h3 id="asr-challenge">ASR Challenge</h3>

<ul>
  <li>Data: LibriSpeech_100 + ML-SUPERBB 1h set</li>
  <li>Framework: We recommend to use ESPnet for fair comparison. Feel free to let us know your preferrence.</li>
  <li>Evaluation metrics: 1) Character Error Rates (CERs) on LibriSpeech and ML-SUPERB evaluation sets; 2) the bitrate of the discrete unit.
    <ul>
      <li>Character Error Rate (CER): This metric measures the performance of a system in terms of the accuracy of the words recognized or generated compared to a reference. All systems are ranked based on the CERs of the evaluation sets separately: 1) EN: dev-clean / dev-other / test-clean / test-other; 2) ML: test-1h. Note that the ranking of the EN case is based on the micro-average CER of all Librispeech test sets (i.e., total errors of {dev,test}-{clean,other}) / (total length of {dev,test}-{clean,other}).</li>
      <li>Bitrate of the discrete unit: This metric measures the efficiency of the discrete representation. Considering different sequence reduction methods (e.g., BPE, deduplication, etc.), we estimate bitrate by considering the whole test set (i.e., all librispeech evaluation sets and ML-SUPERB test sets). Denote the discrete token as \(\{\mathbf{S}_1, ..., \mathbf{S}_M\}\) where \(\mathbf{S}_i\) is the \(i^{\text{th}}\) stream of discrete token and \(M\) is the number of streams. We then define the vocabulary size of \(i^{\text{th}}\) stream as \(V_i\) and the length of \(i^{\text{th}}\) stream as \(L_i\). Considering the total length of test sets as \(N\) (in seconds), the bitrate of the discrete token \(B\) is defined as \(B = \sum_{i=1}^M(L_i / N * \text{log}_2(V_i))\)</li>
    </ul>
  </li>
  <li>Ranking: The overall ranking is based on the Average Rank, which is the average of all three ranking positions:
    <ul>
      <li>R1: micro average CER on all LibriSpeech evaluation sets;</li>
      <li>R2: CER on ML-SUPERB test set;</li>
      <li>R3: the bitrate of the overall test sets.</li>
    </ul>

    <p>The overall ranking position is (R1 + R2 + R3) / 3. If more than 1 systems have the same overall ranking position, they are further ranked by the CER of the test-1h (Please see FAQ section for a detailed example of the ranking).</p>
  </li>
  <li>Submission package details:
    <ol>
      <li>The vocabulary file of input, which includes all possible input token types and special tokens (<code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;eos&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;blank&gt;</code>, etc) within a json format. The key is the order of input streams, while the value is the token list corresponding to the key. Example out:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
 0: ["0", "1", "2", ...],
 1: ["100", "101", "102", ...],
 ...
  }
</code></pre></div>        </div>
      </li>
    </ol>

    <p>In the case of baseline, you may convert the vocab file at <code class="language-plaintext highlighter-rouge">data/token_list/src_bpe_unigram3000_rm_wavlm_largge_21_km2000/bpe.vocab</code>  to an example output like:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
    0: ["&lt;unk&gt;", "&lt;s&gt;", "&lt;/s&gt;", "么", "喤", "佨", "叡", ...]
  }
</code></pre></div>    </div>

    <ol>
      <li>
        <p>The input discrete speech units corresponding to the test sets in a json format. The key of the submission file is the utterance id (refer to ESPnet recipe), the value is a two-dimensional list, with each list corresponds to a stream of discrete tokens.</p>

        <p>E.g. if you follow the baseline and use bpe, the input file can be derived by <code class="language-plaintext highlighter-rouge">paste &lt;(cut -f1 -d" " dump/raw/test_1h/text.rm.wavlm_large_21_km2000) &lt;(spm_encode --model=data/token_list/src_bpe_unigram3000_rm_wavlm_large_21_km2000/bpe.model --output_format=id &lt;dump/raw/test_1h/text.rm.wavlm_large_21_km2000) &gt; output</code>. Example output (baseline):</p>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
 "AccentedFrenchOpenSLR57_fra_000005": [[784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600]]
  }
</code></pre></div>        </div>
      </li>
    </ol>

    <p>If you are using multiple streams, the other streams would be in additional list. Example output:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
    "AccentedFrenchOpenSLR57_fra_000005": [
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
        ]
  }
</code></pre></div>    </div>
    <p>Noted that the token number in each stream is not necessararily the same to each other (e.g., the first stream may have a resolution of 20ms, but the second may be 40ms etc.)</p>

    <ol>
      <li>The predicted transcription corresponding to the test sets.</li>
      <li>A technical report in Interspeech2024 paper format (no length limit, can be submitted one week after the Interspeech submission deadline (i.e., 2024/03/18 AOE))</li>
    </ol>
  </li>
</ul>

<h3 id="tts-challenge---acousticvocoder">TTS Challenge - Acoustic+Vocoder</h3>

<ul>
  <li>Data: LJSpeech, following the train-dev-test split in <a href="https://github.com/ftshijt/Interspeech2024_DiscreteSpeechChallenge">here</a>.</li>
  <li>Framework: No framework or model restriction in the TTS-Acoustic+Vocoder challenge, but the organizers have prepared the baseline training scripts (baseline model to be released soon) in <a href="https://github.com/espnet/espnet/tree/tts2/egs2/ljspeech/tts2">ESPnet</a>.</li>
  <li>Evaluation metrics: Mean cepstral distortion, F0 root mean square error, Bitrate, <a href="https://github.com/sarulab-speech/UTMOS22/tree/master">UTMOS</a>
    <ul>
      <li>Bitrate of the discrete unit: This metric measures the efficiency of the discrete representation. Considering different sequence reduction methods (e.g., BPE, deduplication, etc.), we estimate bitrate by considering the LJSpeech test set (according to the official split provided in the challenge). Denote the discrete token as \(\{\mathbf{S}_1, ..., \mathbf{S}_M\}\) where \(\mathbf{S}_i\) is the \(i^{\text{th}}\) stream of discrete token and \(M\) is the number of streams. We then define the vocabulary size of \(i^{\text{th}}\) stream as \(V_i\) and the length of \(i^{\text{th}}\) stream as \(L_i\). Considering the total length of test sets as \(N\) (in seconds), the bitrate of the discrete token \(B\) is defined as \(B = \sum_{i=1}^M(L_i / N * \text{log}_2(V_i))\)</li>
    </ul>
  </li>
  <li>Ranking: The overall ranking is based on the Average Rank, which is the average of two ranking positions:
    <ul>
      <li>R1: UTMOS;</li>
      <li>R2: the bitrate of the overall test sets.</li>
    </ul>

    <p>The overall ranking position is (R1 + R2) / 2. If more than 1 systems have the same overall ranking position, they are further ranked by R1 (Please see FAQ section for a detailed example of the ranking).</p>
  </li>
  <li>Submission
    <ul>
      <li>Submission package details:
        <ol>
          <li>The synthesized voice of LJSpeech test set using full training set (with at least 16kHz, in zipped format).</li>
          <li>The synthesized voice of LJSpeech test set using 1-hour training set (with at least 16kHz, in zipped format).</li>
          <li>The input discrete speech units corresponding to the test set in a json format. The key of the submission file is the utterance id (refer to the ID in data-split repo), the value is a two-dimensional list, with each list corresponds to a stream of discrete tokens.</li>
        </ol>

        <p>Example output (baseline):</p>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
 "LJ050-0029": [[784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600]]
  }
</code></pre></div>        </div>
      </li>
    </ul>

    <p>If you are using multiple streams, the other streams would be in additional list. Example output:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
    "LJ050-0029": [
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
        ]
  }
</code></pre></div>    </div>
    <p>Noted that the token number in each stream is not necessararily the same to each other (e.g., the first stream may have a resolution of 20ms, but the second may be 40ms etc.)</p>
    <ol>
      <li>A technical report in Interspeech2024 paper format (no length limit, can be submitted one week after the Interspeech submission deadline (i.e., 2024/03/18 AOE))</li>
    </ol>
  </li>
</ul>

<h3 id="tts-challenge---vocoder">TTS Challenge - Vocoder</h3>

<ul>
  <li>Data: Expresso, following the train-dev-test split in <a href="https://github.com/ftshijt/Interspeech2024_DiscreteSpeechChallenge">here</a> (Note that this is different from the original train-dev-test split in the benchmark paper).</li>
  <li>Framework: No framework or model restriction in TTS-Vocoder challenge, but the organizers have prepared the baseline training scripts (baseline model to be released soon) in <a href="https://github.com/espnet/espnet/tree/tts2/egs2/ljspeech/tts2">ESPnet</a> and <a href="https://github.com/kan-bayashi/ParallelWaveGAN">ParallelWaveGAN</a>.</li>
  <li>Evaluation metrics: Mean cepstral distortion, F0 root mean square error, Bitrate, <a href="https://github.com/sarulab-speech/UTMOS22/tree/master">UTMOS</a>
    <ul>
      <li>Bitrate of the discrete unit: This metric measures the efficiency of the discrete representation. Considering different sequence reduction methods (e.g., BPE, deduplication, etc.), we estimate bitrate by considering the Expresso test set (according to the official split provided in the challenge). Denote the discrete token as \(\{\mathbf{S}_1, ..., \mathbf{S}_M\}\) where \(\mathbf{S}_i\) is the \(i^{\text{th}}\) stream of discrete token and \(M\) is the number of streams. We then define the vocabulary size of \(i^{\text{th}}\) stream as \(V_i\) and the length of \(i^{\text{th}}\) stream as \(L_i\). Considering the total length of test sets as \(N\) (in seconds), the bitrate of the discrete token \(B\) is defined as \(B = \sum_{i=1}^M(L_i / N * \text{log}_2(V_i))\)</li>
    </ul>
  </li>
  <li>Ranking: The overall ranking is based on the Average Rank, which is the average of two ranking positions:
    <ul>
      <li>R1: UTMOS;</li>
      <li>R2: the bitrate of the overall test sets.</li>
    </ul>

    <p>The overall ranking position is (R1 + R2) / 2. If more than 1 systems have the same overall ranking position, they are further ranked by R1.</p>
  </li>
  <li>Submission
    <ul>
      <li>Submission package details:
        <ol>
          <li>The synthesized voice of Expresso test set using full training set (with at least 16kHz, in zipped format).</li>
          <li>The input discrete speech units corresponding to the test set in a json format. The key of the submission file is the utterance id (refer to the ID in data-split repo), the value is a two-dimensional list, with each list corresponds to a stream of discrete tokens.</li>
        </ol>

        <p>Example output (baseline):</p>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
 "ex01_confused_00001": [[784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600]]
  }
</code></pre></div>        </div>
      </li>
    </ul>

    <p>If you are using multiple streams, the other streams would be in additional list. Example output:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
    "ex01_confused_00001": [
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
        ]
  }
</code></pre></div>    </div>
    <p>Noted that the token number in each stream is not necessararily the same to each other (e.g., the first stream may have a resolution of 20ms, but the second may be 40ms etc.)</p>
    <ol>
      <li>A technical report in Interspeech2024 paper format (no length limit, can be submitted one week after the Interspeech submission deadline (i.e., 2024/03/18 AOE))</li>
    </ol>
  </li>
</ul>

<h3 id="svs-challenge">SVS Challenge</h3>

<ul>
  <li>Data: Opencpop, following the original segmentation and train/test split.</li>
  <li>Framework: No framework or model restriction in the SVS challenge, but the organizers have prepared the baseline training scripts (baseline model to be released soon) in <a href="https://github.com/A-Quarter-Mile/espnet/tree/tmp_muskit/egs2/opencpop/svs2">ESPnet-Muskits</a>.</li>
  <li>Evaluation metrics
    <ul>
      <li>Objective metrics: Mean cepstral distortion, F0 root mean square error, Bitrate for efficiency measure
        <ul>
          <li>Bitrate of the discrete unit: This metric measures the efficiency of the discrete representation. Considering different sequence reduction methods (e.g., BPE, deduplication, etc.), we estimate bitrate by considering the Opencpop test set (according to the official split provided in the challenge). Denote the discrete token as \(\{\mathbf{S}_1, ..., \mathbf{S}_M\}\) where \(\mathbf{S}_i\) is the \(i^{\text{th}}\) stream of discrete token and \(M\) is the number of streams. We then define the vocabulary size of \(i^{\text{th}}\) stream as \(V_i\) and the length of \(i^{\text{th}}\) stream as \(L_i\). Considering the total length of test sets as \(N\), the bitrate of the discrete token \(B\) is defined as \(B = \sum_{i=1}^M(L_i / N * \text{log}_2(V_i))\)</li>
        </ul>
      </li>
      <li>Subjective metrics: Mean Opinion Score (MOS) by organizers</li>
    </ul>
  </li>
  <li>Ranking: The overall ranking is based on the Average Rank, which is the average of two ranking positions:
    <ul>
      <li>R1: MOS;</li>
      <li>R2: the bitrate of the overall test sets.</li>
    </ul>

    <p>The overall ranking position is (R1 + R2) / 2. If more than 1 systems have the same overall ranking position, they are further ranked by R1 (Please see FAQ section for a detailed example of the ranking).</p>
  </li>
  <li>Submission
    <ul>
      <li>Submission package details:
        <ol>
          <li>The synthesized voice of Opencpop test set using full training set (with at least 16kHz, in zipped format).</li>
          <li>The input discrete speech units corresponding to the test set in a json format. The key of the submission file is the utterance id (refer to the ID in data-split repo), the value is a two-dimensional list, with each list corresponds to a stream of discrete tokens.</li>
        </ol>

        <p>Example output (baseline):</p>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
 "ex01_confused_00001": [[784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600]]
  }
</code></pre></div>        </div>
      </li>
    </ul>

    <p>If you are using multiple streams, the other streams would be in additional list. Example output:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  {
    "ex01_confused_00001": [
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
            [784, 0, 1953, 1126, 9, 1126, 547, 273, 443, 541, 16, 768, 10, 806, 1380, 1151, 61, 382, 1004, 765, 2162, 1698, 128, 2621, 357, 914, 480, 715, 89, 1369, 893, 1307, 266, 64, 266, 681, 828, 641, 689, 1026, 488, 448, 182, 860, 1552, 628, 233, 1156, 22, 438, 659, 2239, 1125, 888, 22, 888, 1493, 752, 283, 123, 1296, 266, 64, 1000, 1187, 548, 1481, 671, 318, 629, 652, 89, 312, 1451, 88, 1826, 504, 1588, 145, 1296, 266, 64, 542, 340, 1805, 651, 217, 962, 1519, 229, 10, 403, 600],
        ]
  }
</code></pre></div>    </div>
    <p>Noted that the token number in each stream is not necessararily the same to each other (e.g., the first stream may have a resolution of 20ms, but the second may be 40ms etc.)</p>
    <ol>
      <li>A technical report in Interspeech2024 paper format (no length limit, can be submitted one week after the Interspeech submission deadline (i.e., 2024/03/18 AOE))</li>
    </ol>
  </li>
</ul>

<h3 id="research-in-the-discrete-representation-of-speech-and-audio">Research in the discrete representation of speech and audio</h3>

<ul>
  <li>Call for research papers: As a special session, the track also accepts research papers in discrete representation of speech and audio. The paper could be related to any of the following topics:
    <ul>
      <li>Discrete speech/audio/music representation learning</li>
      <li>Discrete representation application for any speech/audio processing downstream tasks (ASR, TTS, etc.)</li>
      <li>Evaluation of speech/audio discrete representation</li>
      <li>Efficient discrete speech/audio discrete representation</li>
      <li>Interpretability in discrete speech/audio discrete representation</li>
      <li>Other novel usage of discrete representation in speech/audio</li>
    </ul>
  </li>
  <li>Please refer to the “Paper submission” section for detailed guidance on paper submission.</li>
</ul>

<h2 id="paper-submission">Paper submission</h2>

<p>Papers for the Interspeech Special Session have to be submitted following the same schedule and procedure as regular papers of <a href="https://interspeech2024.org/">INTERSPEECH 2024</a>. The submitted papers will undergo the same review process by anonymous and independent reviewers.</p>

<p>Please use the <a href="https://cmt3.research.microsoft.com/INTERSPEECH2024">submission URL</a>) to submit papers and select “14.10 Speech Processing Using Discrete Speech Units (Special Session)” as the primary subject areas for your paper submission.</p>

<p>For techincal reports, please submit the paper via the google form: https://forms.gle/K7fehBcoVEMXB9tx9</p>

<h2 id="submission-pakcage-submission">Submission pakcage submission</h2>

<p>Participants will need to submit their submission package through the <a href="https://forms.gle/XAdQ5WyEVD2tDwYh8">google form</a>, where the leaderboard will be updated accordingly within 48 hours. The final ranking information will be recorded at the time of Interspeech submission deadline (i.e., 2024/03/11 AOE).</p>

<p>Each team will only be able to submit up-to 5 systems to the leaderboard.</p>

<p>For SVS track, because of the limited budget and the requirements of extra evaluation time for the subjective evaluation, up-to 3 systems are accepted to the leaderboard and deadline for final submissions is 2024/03/05 AOE. While the bitrate evaluation in the leaderboard will be updated within 48 hours, the MOS leaderboard will be updated before 2024/03/10 AOE.</p>

<p>The leaderboard is online at https://huggingface.co/discrete-speech.</p>

<!---
### Schedules
--->

<h2 id="important-dates">Important dates</h2>
<p>The schedule for the challenge is as follows</p>
<ul>
  <li>Feb 20, 2024: <a href="https://huggingface.co/discrete-speech">Leaderboard</a> is online and accepting submissions</li>
  <li>Mar  2, 2024: Paper Submission Deadline</li>
  <li>Mar  11, 2024: Paper Revision Deadline</li>
  <li>After Mar. 11: The Leaderboard will still be open and new submissions will be evaluated</li>
</ul>

<h2 id="faq">FAQ</h2>
<ul>
  <li>For each track, you have shown a train set. Is the data used for each track limited to those datasets? Or can we use other datasets such as librilight. If the dataset used for training is limited to the one shown on the website, can we use pretrained models such as Whisper or llama2?
    <ul>
      <li>For discrete representation/units extraction, we do not have requirements of the data to use, so you may use any of the models you mentioned such as Librilight, or pre-trained models such as Whisper or Llama2. (But to make sure that the supervision leakage, we do not allow you to use supervision data in the track test data; For example, you cannot use Librispeech test data and ML-SUPERB test data with their labels for discrete representation extraction purposes.)</li>
    </ul>
  </li>
  <li>Can we use additional information such as text/phoneme sequence for vocoders in TTS tracks?
    <ul>
      <li>For the TTS (acoustic+vocoder) track, you can use text/phoneme sequence in the vocoder. However, for the TTS (vocoder) track, you can only use discrete representations, where the discrete representation can be only extracted from the waveform.</li>
    </ul>
  </li>
  <li>Can we use additional information such as phone, duration, and note sequences for vocoders in the SVS track
    <ul>
      <li>Yes, you can use the music score information in the vocoder of SVS systems.</li>
    </ul>
  </li>
  <li>Can you provide the evaluation scripts of the TTS/SVS objective metrics?
    <ul>
      <li>We will use the scripts in https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1#evaluation for objective metrics.</li>
    </ul>
  </li>
  <li>What does it mean for “No framework or model restrictions in TTS/SVS”?
    <ul>
      <li>As we offered baselines in ESPnet, we do not have any requirements for only using ESPnet. In short, you may use any toolkits (e.g., coqui-TTS, speechbrain, etc.) or any models (Tacotron, Fastspeech, diffusion-based models, decoder-only AR models such as Vall-E or spearTTS) for the purpose, as long as you follow the other guidelines in the challenge.</li>
    </ul>
  </li>
  <li>‘Submission package details’ in the TTS vocoder challenge says “with at least 16kHz”. However, the Expresso dataset is 48 kHz. Should I conduct experiments at 48kHz, or is it acceptable to conduct experiments at any sampling rate greater than 16kHz?
    <ul>
      <li>For target audio, we will do a resample to 16kHz if participants submit &gt;16kHz audio (that is mostly because the evaluation metrics (e.g., WER/UTMOS) are performed on 16kHz audio-only.</li>
    </ul>
  </li>
  <li>Will the organizers also consider other metrics for the evaluation (especially for TTS and SVS)?
    <ul>
      <li>We may add additional metrics for participants’ reference. However, we will stick to the current ranking metrics for now to keep it fair for all the participants.</li>
    </ul>
  </li>
  <li>Can the participants use additional information for training the TTS acoustic model (such as use Mel spectrogram to train VAE or duration information to train fastspeech-like models)?
    <ul>
      <li>Yeah, additional information from the audio can be used for the TTS acoustic model as long as the output of TTS acoustic model is in discrete space.</li>
    </ul>
  </li>
  <li>Do you have an example of the rankings?
    <ul>
      <li>Take ASR as an example, if system A ranks 1st place in R1, 2nd place in R2, 3rd place in R3; system B ranks 3rd place in R1, 1nd place in R2, 2nd place in R3, the overall ranking positions for system A and B are both <code class="language-plaintext highlighter-rouge">2</code>. However, considering the rank in R2, system B would have a better final ranking.</li>
    </ul>
  </li>
  <li>Why there are two deadlines for the paper submission, what are their differences?
    <ul>
      <li>We have two kinds of submission available, which you may select from:
        <ul>
          <li>Submit to Interspeech as a regular paper (for the special session, need to select discrete speech challenge as the primary topic):
            <ul>
              <li>for this option, the deadline is the same as Interspeech paper. Noted that the reviewing process is the same as the regular Interspeech paper (and will be in the proceedings as regular paper). And you can include this submission as your system description paper for the challenge summary in the submission package via google form. If you select this option, the deadline is March 2, 2024 for abstract and revision deadline is March 11, 2024.</li>
            </ul>
          </li>
          <li>Submit via google form as the system description paper:
            <ul>
              <li>For this option, you will do not need to submit to the Interspeech portal, but just include your system description paper in the google form (noted that the paper will not be in the proceedings, in other words, no peer review). This paper will be used for the challenge summary (selected system description papers might be provided chances to be presented in the special session). If you select only this option, the deadline is March 18, 2024.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<!---
### Organizers
--->

<h2 id="organizers">Organizers</h2>

<ul>
  <li>Xuankai Chang (Carnegie Mellon University, U.S.)</li>
  <li>Jiatong Shi (Carnegie Mellon University, U.S.)</li>
  <li>Shinji Watanabe (Carnegie Mellon University, U.S.)</li>
  <li>Yossi Adi (Hebrew University, Israel)</li>
  <li>Xie Chen (Shanghai Jiao Tong University, China)</li>
  <li>Qin Jin (Renmin University of China, China)</li>
</ul>

<h2 id="contact">Contact</h2>
<ul>
  <li>discrete_speech_challenge@googlegroups.com</li>
</ul>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025   WAV Lab.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
