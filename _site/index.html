<!DOCTYPE html>
<html lang="">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->


<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    WAVLab
  
</title>
<meta name="author" content="  WAV Lab">
<meta name="description" content="affiliated with @ &lt;a href=" https:>
</head>
<body>
<p>LTI/CMU."&gt;










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->
<link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">




<!-- Styles -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%97%A3%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/">

<!-- Dark Mode -->

  <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script>


  

  <!-- Body -->
  </p>
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/courses/">Courses
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/info/">Info
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/members/">Members
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/open_source">Open-source
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/positions/">Positions
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/publications/">Publications
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/sphinx_lunch">Sphinx Lunch
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/sponsors/">Sponsors
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="fa-solid fa-moon"></i>
                <i class="fa-solid fa-sun"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">
      
        WAVLab
      
    </h1>
    <p class="desc"></p>
  </header>

  <article>
    

    <div class="clearfix">
<p>This is Watanabe’s Audio and Voice (WAV) Lab at the Language Technologies Institute of Carnegie Mellon University. Our research interests include automatic speech recognition, speech enhancement, spoken language understanding, and machine learning for speech and language processing.</p>

<div class="col-sm mt-3 mt-md-0" style="display:table-cell; vertical-align:middle; text-align:center">
    <img class="img-fluid rounded z-depth-1" src="/assets/img/gallery/03-06-2022.jpg">
    <div class="caption">
        Our Lab Party at the Porch before Interspeech, 03.06.2022
    </div>
</div>

<!-- <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-tweet-limit="3" href="https://twitter.com/WavLab?ref_src=twsrc%5Etfw">Tweets by WavLab</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div> -->

<!-- # Tweet
An example of displaying a tweet:
<div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div> -->
</div>

    <!-- News -->
    
      <h2>
        <a href="/news/" style="color: inherit">news</a>
      </h2>
      <div class="news">
  
    
    <div class="table-responsive" style="max-height: 60vw">
      <table class="table table-sm table-borderless">
        
        
        
          <tr>
            <th scope="row" style="width: 20%">Jun 15, 2022</th>
            <td>
              
                Our lab has 23 paper accepted in the Interspeech2022. Detailed list will be available in <a href="/publications">publication page</a>.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row" style="width: 20%">Feb 28, 2022</th>
            <td>
              
                Our lab has 18 ICASSP paper accepted in the ICASSP2022. Detailed list is already available in <a href="/publications">publication page</a>.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row" style="width: 20%">Sep 13, 2021</th>
            <td>
              
                Our lab has 9 ASRU paper accepted in the ASRU2021. Detailed list is already available in <a href="/publications">publication page</a>.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row" style="width: 20%">Jun 07, 2021</th>
            <td>
              
                Shinji, with Keisuke, Yusuke, and Naoyuki, delivered a tutorial on “Distant Conversational Speech Recognition And Analysis: Recent Advances, And Trends Towards End-To-End Optimization” in ICASSP 2021. Detailed slides can be found <a href="https://github.com/ICASSP2021-tutorial9/Distant_conversational_ASR_and_analysis" rel="external nofollow noopener" target="_blank">here</a>.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row" style="width: 20%">Jun 03, 2021</th>
            <td>
              
                Our lab has 20 Interspeech paper accepted in the Interspeech2021. Detailed list will be available soon in <a href="/publications">publication page</a>.

              
            </td>
          </tr>
        
      </table>
    </div>
  
</div>

    

    <!-- Latest posts -->
    

    <!-- Selected papers -->
    
      <h2>
        <a href="/publications/" style="color: inherit">selected publications</a>
      </h2>
      <div class="publications">
  <ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">SD</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="park2022review" class="col-sm-8">
    <!-- Title -->
    <div class="title">A review of speaker diarization: Recent advances with deep learning</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Tae Jin
            Park
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Naoyuki
            Kanda
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Dimitrios
            Dimitriadis
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Kyu J
            Han
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Shrikanth
            Narayanan
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Computer Speech &amp; Language</em>,  2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">ASR&amp;SD&amp;SLU&amp;ER</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="yang2021superb" class="col-sm-8">
    <!-- Title -->
    <div class="title">SUPERB: Speech processing Universal PERformance Benchmark</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Shu-wen
            Yang
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Po-Han
            Chi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yung-Sung
            Chuang
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Cheng-I
            Lai
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Kushal
            Lakhotia
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yist
            Y.
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Andy
            T.
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Jiatong
            Shi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Xuankai
            Chang
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Guan-Ting
            Lin
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Tzu-Hsien
            Huang
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Wei-Cheng
            Tseng
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Ko-tik
            Lee
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Da-Rong
            Liu
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Zili
            Huang
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shuyan
            Dong
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shang-Wen
            Li
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Abdelrahman
            Mohamed
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Hung-yi
            Lee
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of Interspeech</em> ,  2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a href="http://arxiv.org/abs/2105.01051" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
        <a href="https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://arxiv.org/pdf/2105.01051.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">TTS</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="hayashi2020espnet" class="col-sm-8">
    <!-- Title -->
    <div class="title">Espnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Tomoki
            Hayashi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Ryuichi
            Yamamoto
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Katsuki
            Inoue
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Takenori
            Yoshimura
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Tomoki
            Toda
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Kazuya
            Takeda
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yu
            Zhang
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Xu
            Tan
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> ,  2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a href="http://arxiv.org/abs/1910.10909" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
        <a href="https://ieeexplore.ieee.org/abstract/document/9053512/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://arxiv.org/pdf/1910.10909.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">ST</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="inaguma-etal-2020-espnet" class="col-sm-8">
    <!-- Title -->
    <div class="title">ESPnet-ST: All-in-One Speech Translation Toolkit</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Hirofumi
            Inaguma
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shun
            Kiyono
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Kevin
            Duh
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shigeki
            Karita
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Nelson
            Yalta
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Tomoki
            Hayashi
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Shinji
            Watanabe
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics</em> ,  Jul 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">SD</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="horiguchi2020end" class="col-sm-8">
    <!-- Title -->
    <div class="title">End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Shota
            Horiguchi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yusuke
            Fujita
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yawen
            Xue
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Kenji
            Nagamatsu
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em></em> Jul 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a href="http://arxiv.org/abs/2005.09921" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
        <a href="https://github.com/hitachi-speech/EEND" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">SD</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="fujita2019end" class="col-sm-8">
    <!-- Title -->
    <div class="title">End-to-end neural speaker diarization with self-attention</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Yusuke
            Fujita
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Naoyuki
            Kanda
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shota
            Horiguchi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yawen
            Xue
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Kenji
            Nagamatsu
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Shinji
            Watanabe
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In IEEE Automatic Speech Recogiton and Understanding Workshop (ASRU)</em> ,  Jul 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a href="http://arxiv.org/abs/1909.06247" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
        <a href="https://ieeexplore.ieee.org/abstract/document/9003959" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://arxiv.org/pdf/1909.06247.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">SD</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="fujita19inter" class="col-sm-8">
    <!-- Title -->
    <div class="title">End-to-End Neural Speaker Diarization with Permutation-Free Objectives</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Yusuke
            Fujita
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Naoyuki
            Kanda
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shota
            Horiguchi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Kenji
            Nagamatsu
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Shinji
            Watanabe
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of Interspeech</em> ,  Jul 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
      
      
        <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/2899.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2899.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">ASR</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="karita19inter" class="col-sm-8">
    <!-- Title -->
    <div class="title">Improving Transformer Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Shigeki
            Karita
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Nelson
            Yalta
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Marc
            Delcroix
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Atsunori
            Ogawa
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Tomohiro
            Nakatani
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of Interspeech</em> ,  Jul 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
      
      
        <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/abstracts/1938.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/1938.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">ASR</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="watanabe2018espnet" class="col-sm-8">
    <!-- Title -->
    <div class="title">ESPnet: End-to-End Speech Processing Toolkit</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Takaaki
            Hori
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shigeki
            Karita
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Tomoki
            Hayashi
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Jiro
            Nishitoba
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Yuya
            Unno
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Nelson
            Enrique Yalta Soplin
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Jahn
            Heymann
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Matthew
            Wiesner
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Nanxin
            Chen
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Adithya
            Renduchintala
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Tsubasa
            Ochiai
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Proceedings of Interspeech</em>,  Jul 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/1804.00015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
        <a href="https://www.isca-speech.org/archive/interspeech_2018/watanabe18_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/watanabe18_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/espnet/espnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">SE&amp;ASR</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="barker2018fifth" class="col-sm-8">
    <!-- Title -->
    <div class="title">The Fifth ’CHiME’ Speech Separation and Recognition Challenge: Dataset, Task and Baselines</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Jon
            Barker
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Emmanuel
            Vincent
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            Jan
            Trmal
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Proceedings of Interspeech</em>,  Jul 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a href="http://arxiv.org/abs/1803.10609" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
        <a href="https://www.isca-speech.org/archive/interspeech_2018/barker18_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/barker18_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          <abbr class="badge">SD</abbr>
        
      
    </div>
  


  <!-- Entry bib key -->
  <div id="sell2018diarization" class="col-sm-8">
    <!-- Title -->
    <div class="title">Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge.</div>
    <!-- Author -->
    <div class="author">
      

      
      

      
        
        
        
        
        
        

        
          
          
        
        
          
            Gregory
            Sell
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            David
            Snyder
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Alan
            McCree
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Daniel
            Garcia-Romero
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Jesús
            Villalba
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Matthew
            Maciejewski
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Vimal
            Manohar
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Najim
            Dehak
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Daniel
            Povey
          
        
      
        
        
        
        
        
        

        
          , 
          
        
        
          
            Shinji
            Watanabe
          
        
      
        
        
        
        
        
        

        
          , 
          and 
        
        
          
            
            others
          
        
      
      

      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Interspeech</em> ,  Jul 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
        <a href="https://www.isca-speech.org/archive/interspeech_2018/sell18_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
      
      
        
          <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/sell18_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We describe in this paper the experiences of the Johns Hopkins University team during the inaugural DIHARD diarization evaluation. This new task provided microphone recordings in a variety of difficult conditions and challenged researchers to fully consider all speaker activity, without the currently typical practices of unscored collars or ignored overlapping speaker segments. This paper explores several key aspects of currently state-of-the-art diarization methods, such as training data selection, signal bandwidth for feature extraction, representations of speech segments (i-vector versus x-vector) and domain-adaptive processing. In the end, our best system clustered x-vector embeddings trained on wideband microphone data followed by Variational-Bayesian refinement and a speech activity detector specifically trained for this task with in-domain data was found to be the best performing. After presenting these decisions and their final result, we discuss lessons learned and remaining challenges within the lens of this new approach to diarization performance measurement.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
</div>

    

    <!-- Social -->
    
      <div class="social">
        <div class="contact-icons">











  <a href="https://github.com/shinjiwlab" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a>


  <a href="https://www.linkedin.com/in/wavlab-cmu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a>

























  <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a>


</div>

        <div class="contact-note">You can even add a little note about which of these is the best way to reach you.
</div>
      </div>
    
  </article>
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=s2kcUarMcOgurEYClUOjmjuZwmzMNsQOUdYJfD1PwGY&amp;cl=ffffff&amp;w=a"></script>
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      
      
      WAV Lab. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->
<script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script>
<script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script>
<script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
      },
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

  
</body>
</html>
