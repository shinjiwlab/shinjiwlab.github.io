---
layout: post
title: TIR Usage
date: 2022-01-01 09:00:00-0800
description: CMU TIR cluster usage.
comments: false
---

# Installation

## Starting a bash Slurm job
```bash
srun --pty -n 1 --cpus-per-task=8  --gres=gpu:1 --mem=12G /bin/bash -l
```

## Loading Required Modules
```bash
module load cuda-11.1.1 cudnn-11.1.1-v8.0.4.30 gcc-5.5.0
```

## Miniconda installation
   ```bash
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh
   ```

## Installing ESPnet & Kaldi

```bash
cd <path-to-your-projects>
git clone https://github.com/espnet/espnet

cd <espnet-root>/tools/
git clone https://github.com/kaldi-asr/kaldi

# setup virtual environment (venv) for python
cd <espnet-root>/tools/
./setup_venv.sh $(command -v python3)
```

### Building kaldi
```bash
cd <espnet-root>/tools/
. activate_python.sh
```

Check dependencies and install OpenBLAS (MKL and ATLAS installations need sudo privileges)
```bash
cd <kaldi-root>/tools/
extras/check_dependencies.sh
make -j 8
./extras/install_openblas.sh
./extras/install_irstlm.sh

cd <kaldi-root>/src
# without CUDA (ESPnet uses only feature extractor, so you can disable CUDA)
./configure --openblas-root=../tools/OpenBLAS/install --use-cuda=no
make -j clean depend; make -j 8
```

### Building espnet
```bash
cd <espnet-root>/tools
make -j 8 CUDA_VERSION=11.1 TH_VERSION=1.8.1
```

### Check espnet + kaldi installation
```bash
cd <espnet-root>/egs/an4/asr1/
./run.sh
```

## Exit the bash Slurm job
```bash
exit
```

## Misc.
### Installing sox manually (if unable to install via conda)
```bash
mkdir -p ~/rpm
cd ~/rpm
wget http://mirror.centos.org/centos/7/os/x86_64/Packages/sox-14.4.1-7.el7.x86_64.rpm
rpm2cpio ~/rpm/sox-14.4.1-7.el7.x86_64.rpm | cpio -id

export PATH="$HOME/rpm/usr/sbin:$HOME/rpm/usr/bin:$HOME/rpm/bin:$PATH"
L='/lib:/lib64:/usr/lib:/usr/lib64'
export LD_LIBRARY_PATH="$L:$HOME/rpm/usr/lib:$HOME/rpm/usr/lib64"
```
Note: You can also add the last 3 lines to your `~/.bashrc` file because connecting over ssh reads and executes commands from `~/.bashrc`.

#### Install Sox from scratch with flac support: refer to [this](http://akshayc.com/blog/build-sox-from-scratch/).


# Login
Simple `ssh` would be working:

```
ssh <your_username>@tir.lti.cs.cmu.edu
```

# Usage details

## General guidelines
There is a general document for tir usage at https://docs.google.com/document/d/1ieMgNos6F97XAtfD_m6WINte1gAQcPDqpqbbB82Rv4A/edit?usp=sharing

## Data storage
We have stored many databases in `/projects/tir5/data/speech_corpora`. Please look ahead at the directory before downloading on your own. In the same time, please add new databases there if you have any other needs.

## IO issues
The TIR may have IO issues when directly train models with data stored in storage node. One option to fix that is to shift the prepared features to the `/tmp/` and then run my training. After that delete related files from /tmp before exit.

### Procedures for ESPNet1 would like:
```
dumpdir=`mktemp -d /tmp/st-XXXX`    # directory to dump full features

feat_tr_dir=${dumpdir}/${train_set}/delta${do_delta}; mkdir -p ${feat_tr_dir}
feat_dt_dir=${dumpdir}/${train_dev}/delta${do_delta}; mkdir -p ${feat_dt_dir}
```

### Procedures for ESPNet2 would like:
In `run.sh`, set audio format to lower IO issues
```
--audio_format "flac.ark" \
```

At the start of the training stage (e.g., `asr.sh` stage 11), add:
```
tempdir=$(mktemp -d "/tmp/<your_projectname>-$$.XXXXXXXX")
trap 'rm -rf ${tempdir}' EXIT
cp -r "${data_feats}" ${tempdir}
# or rsync -zav --progress --bwlimit=100 "${data_feats}" ${tempdir}
data_feats="${tempdir}/$(basename ${data_feats})"
scp_lists=$(find ${tempdir} -type f -name "*.scp")
for f in ${scp_lists}; do
    sed -i -e "s/${dumpdir//\//\\/}/${tempdir//\//\\/}/g" $f
done

```

At the end of `asr.sh`, add
```
rm -rf ${tempdir}
```

As the tmp folder is corresponding to specifc compute node, please set the `cmd` as `local` and process `run.sh` with `sbatch`

## Notes for running multiple GPUs
When running jobs with multiple GPUs, you should submit jobs with arguments as `--mem Xgb --cpus-per-task Y --gres gpu:ngpus`. A simple rule would be like 
```
X = 16 * ngpus or 4 * ncpus
Y = 5 * ngpus
```
